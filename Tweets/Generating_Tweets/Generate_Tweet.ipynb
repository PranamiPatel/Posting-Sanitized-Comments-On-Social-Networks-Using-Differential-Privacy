{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec6e65e-f51e-4cde-bde7-71d3f70a72b3",
      "metadata": {
        "id": "fec6e65e-f51e-4cde-bde7-71d3f70a72b3"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3557790c-9205-4081-bb47-e4a030193c43",
      "metadata": {
        "id": "3557790c-9205-4081-bb47-e4a030193c43"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c653d8d6-8e44-4d8c-8a9c-3898e1f4f77b",
      "metadata": {
        "id": "c653d8d6-8e44-4d8c-8a9c-3898e1f4f77b"
      },
      "outputs": [],
      "source": [
        "strong_word_file = pd.read_csv(\"C:/Users/user/Downloads/Project/Tweet/RNN/extracted_strong_word.csv\")\n",
        "original_data = pd.read_csv(\"C:/Users/user/Downloads/Project/Tweet/Tweets Dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8176ab0f-a891-431c-9ef0-96155d802b45",
      "metadata": {
        "id": "8176ab0f-a891-431c-9ef0-96155d802b45"
      },
      "outputs": [],
      "source": [
        "strong_word_file.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44438b53-0e15-438d-8ec5-ad7111ca9c5b",
      "metadata": {
        "id": "44438b53-0e15-438d-8ec5-ad7111ca9c5b"
      },
      "outputs": [],
      "source": [
        "symbols_dict = {\n",
        "    '!': 'Exclamation Mark',\n",
        "    '\"': 'Double Quotation Mark',\n",
        "    '#': 'Hash/Pound Sign',\n",
        "    '$': 'Dollar Sign',\n",
        "    '%': 'Percent Sign',\n",
        "    '&': 'Ampersand',\n",
        "    \"'\": 'Single Quotation Mark',\n",
        "    '(': 'Left Parenthesis',\n",
        "    ')': 'Right Parenthesis',\n",
        "    '*': 'Asterisk',\n",
        "    '+': 'Plus Sign',\n",
        "    ',': 'Comma',\n",
        "    '-': 'Hyphen',\n",
        "    '.': 'Period',\n",
        "    '/': 'Forward Slash',\n",
        "    ':': 'Colon',\n",
        "    ';': 'Semicolon',\n",
        "    '<': 'Less Than Sign',\n",
        "    '=': 'Equal Sign',\n",
        "    '>': 'Greater Than Sign',\n",
        "    '?': 'Question Mark',\n",
        "    '@': 'At Sign',\n",
        "    '[': 'Left Square Bracket',\n",
        "    ']': 'Right Square Bracket',\n",
        "    '^': 'Caret',\n",
        "    '_': 'Underscore',\n",
        "    '`': 'Backtick',\n",
        "    '{': 'Left Curly Brace',\n",
        "    '|': 'Vertical Bar',\n",
        "    '}': 'Right Curly Brace',\n",
        "    '~': 'Tilde',\n",
        "    '...': 'Three Dots'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c526b4eb-1d4a-40c9-8eb4-3cb4689ea266",
      "metadata": {
        "id": "c526b4eb-1d4a-40c9-8eb4-3cb4689ea266"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5946e632-99d1-42f4-bdf8-3a285ec8b9dd",
      "metadata": {
        "id": "5946e632-99d1-42f4-bdf8-3a285ec8b9dd"
      },
      "outputs": [],
      "source": [
        "glove_model = api.load(\"glove-wiki-gigaword-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d626b29-82c0-4a56-908c-70e03e481625",
      "metadata": {
        "id": "8d626b29-82c0-4a56-908c-70e03e481625"
      },
      "outputs": [],
      "source": [
        "def prepare_Vs(comment, label, strong_word_file, n) :\n",
        "    tokens = word_tokenize(str(comment).lower())\n",
        "    Vs, Vn = [], []\n",
        "\n",
        "    for token in tokens :\n",
        "        strong_word = strong_word_file[strong_word_file['Word'] == token]\n",
        "        if not strong_word.empty and strong_word['Label'].values[0] == label :\n",
        "            Vs.append((token, strong_word['limescore'].values[0]))\n",
        "        else :\n",
        "            Vn.append(token)\n",
        "\n",
        "    Vs.sort(key=lambda x: x[1], reverse=True)\n",
        "    Vn.extend([word for word, _ in Vs[n:]])\n",
        "    Vs = [word for word, _ in Vs[:n]]\n",
        "\n",
        "    return Vs, Vn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edbb1494-87b7-4073-8084-3d3f28eb97bf",
      "metadata": {
        "id": "edbb1494-87b7-4073-8084-3d3f28eb97bf"
      },
      "outputs": [],
      "source": [
        "def d_angular(x,y):\n",
        "    dot_product = np.dot(x, y)\n",
        "    norm_x = np.linalg.norm(x)\n",
        "    norm_y = np.linalg.norm(y)\n",
        "\n",
        "    cosine_similarity = dot_product / (norm_x * norm_y)\n",
        "    cosine_similarity = np.clip(cosine_similarity, -1.0, 1.0)\n",
        "    angular_distance_radians = np.arccos(cosine_similarity)\n",
        "\n",
        "    return angular_distance_radians"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55cfdf9a-ee9b-4b9e-a9c2-348d4d4adcd8",
      "metadata": {
        "id": "55cfdf9a-ee9b-4b9e-a9c2-348d4d4adcd8"
      },
      "outputs": [],
      "source": [
        "def compute_probability(x, y, Vp, epsilon):\n",
        "\n",
        "    x_vec = glove_model[x]\n",
        "    y_vec = glove_model[y]\n",
        "    d_angular_sim = d_angular(x_vec, y_vec)\n",
        "\n",
        "    sum_exp = 0\n",
        "    for v in Vp:\n",
        "        if v in glove_model:\n",
        "            sum_exp += np.exp(-0.5 * epsilon * d_angular(x_vec, glove_model[v]))\n",
        "\n",
        "    Cx = 1 / sum_exp if sum_exp != 0 else 1\n",
        "    prob = Cx * np.exp(-0.5 * epsilon * d_angular_sim)\n",
        "    return prob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56430b7d-4d70-45c5-9eb3-b1c35958b33e",
      "metadata": {
        "id": "56430b7d-4d70-45c5-9eb3-b1c35958b33e"
      },
      "outputs": [],
      "source": [
        "# Function to find top N semantically similar words\n",
        "def get_top_similar_words(word, model, top_n):\n",
        "    threshold = 0.6\n",
        "    try:\n",
        "        similar_words = model.most_similar(word, topn=top_n*2)\n",
        "        return [item[0] for item in similar_words if item[1] >= threshold][:top_n]\n",
        "    except KeyError:\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e451798f-e609-4f8f-8388-ccfc132c5a98",
      "metadata": {
        "id": "e451798f-e609-4f8f-8388-ccfc132c5a98"
      },
      "outputs": [],
      "source": [
        "def substitute_word(word, Vp, epsilon):\n",
        "    if not Vp:\n",
        "        return word, 0.0\n",
        "\n",
        "    y = np.random.choice(Vp)\n",
        "    prob = compute_probability(word, y, Vp, epsilon)\n",
        "    return y, prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "204a8f39-182b-4c3d-a681-9e71313c217f",
      "metadata": {
        "id": "204a8f39-182b-4c3d-a681-9e71313c217f"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def generator(glove_model, top_strong_word, top_similar_word, epsilon, output_file):\n",
        "    generated_data = []\n",
        "\n",
        "    p = 0.5\n",
        "    for i in tqdm(range(len(original_data)), desc=\"Generating Tweet\", unit=\"Tweet\"):\n",
        "        comment = original_data.iloc[i]['Tweet']\n",
        "        label = original_data.iloc[i]['Party']\n",
        "        genders = 0 if label == \"Democrat\" else 1\n",
        "\n",
        "        Vs, Vn = prepare_Vs(comment, genders, strong_word_file, top_strong_word)\n",
        "\n",
        "        # Tokenize the comment\n",
        "        tokens = word_tokenize(comment.lower())\n",
        "        new_comment = []\n",
        "\n",
        "        for word in tokens:\n",
        "\n",
        "            if  word in stop_words or any(ord(c) > 127 for c in word) or word in symbols_dict :\n",
        "                new_comment.append(word)\n",
        "\n",
        "            elif word in Vs :\n",
        "                Vg = get_top_similar_words(word, glove_model, top_similar_word)\n",
        "                Vp = list(set(Vg) - set(Vs))\n",
        "\n",
        "                new_word, prob = substitute_word(word, Vp, epsilon)\n",
        "                new_comment.append(new_word)\n",
        "\n",
        "            else:\n",
        "                flip = random.random()\n",
        "                if flip <= p:\n",
        "                    new_comment.append(word)\n",
        "\n",
        "                else:\n",
        "                    Vg = get_top_similar_words(word, glove_model, top_similar_word)\n",
        "                    Vp = list(set(Vg) - set(Vs))\n",
        "\n",
        "                    new_word, prob = substitute_word(word, Vp, epsilon)\n",
        "                    new_comment.append(new_word)\n",
        "\n",
        "        generated_comment = \" \".join(new_comment)\n",
        "\n",
        "        generated_data.append([comment, generated_comment, label])\n",
        "\n",
        "    # Save to CSV\n",
        "    generated_df = pd.DataFrame(generated_data, columns=['Original Tweet', 'Generated Tweet', 'Label'])\n",
        "    generated_df.to_csv(output_file, index=False)\n",
        "    print(f\"Generated comments saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78c6617f-d6c4-42cf-9fe0-e5b1fb271368",
      "metadata": {
        "id": "78c6617f-d6c4-42cf-9fe0-e5b1fb271368"
      },
      "outputs": [],
      "source": [
        "generator(glove_model, 2, 10, 1, \"C:/Users/user/Downloads/Project/Tweet/RNN/Using_angular/comment_2_10_10.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84e20107-ec1a-4096-98e4-1681c0ec392b",
      "metadata": {
        "id": "84e20107-ec1a-4096-98e4-1681c0ec392b"
      },
      "outputs": [],
      "source": [
        "generator(glove_model, 2, 15, 1, \"C:/Users/user/Downloads/Project/Tweet/RNN/Using_angular/comment_2_15_10.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260440fe-6754-4c6b-be30-46fb129de77f",
      "metadata": {
        "id": "260440fe-6754-4c6b-be30-46fb129de77f"
      },
      "outputs": [],
      "source": [
        "generator(glove_model, 3, 10, 1, \"C:/Users/user/Downloads/Project/Tweet/RNN/Using_angular/comment_3_10_10.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708505cf-2983-4da0-ac59-03f698996c4b",
      "metadata": {
        "id": "708505cf-2983-4da0-ac59-03f698996c4b"
      },
      "outputs": [],
      "source": [
        "generator(glove_model, 3, 15, 1, \"C:/Users/user/Downloads/Project/Tweet/RNN/Using_angular/comment_3_15_10.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:anaconda3] *",
      "language": "python",
      "name": "conda-env-anaconda3-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}